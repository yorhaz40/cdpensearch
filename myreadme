原始数据在code_reverse和comments_reverse中
1. commenttfidf.py用tfidf算法生成注释中的关键词序列，作为代码的标签，commenttfidf.txt
2. datapre.py用来整理训练、测试数据，生成(train/test/valid).(docstring/function)
3. model.py用来训练从代码片段到代码标签的模型，存储在model_save中1
4. fastaimodel.py用comments_reverse，即最原始的注释数据，训练自然语言模型，用于将注释转化为向量2
5.　用Model.py将所有test.function里的代码生成其对应的标签
6. 用信息检索的方法，测试从代码注释到代码生成的标签之间的查找准确度(这里的信息检索方法，可以用word2vector，即训练好的模型2试试)（需要找回test集合里都对应的哪些代码注释..数据划分处理时候没存）
6.5 用信息检索的方法，测试从代码注释到代码片段之间的查找准确度
7. 训练从代码片段到原代码注释的模型3
8. 提取3的encoder部分，生成代码向量，与2的语言模型生成的向量做对比，进行查找，查看精确度



6. 基本准确度测试：　
输入：　test数据集中的代码注释
查找范围：整个test数据集
查看排名前５的搜索结果，如果包含了target，则认为找到了。计算全部找到的个数除以总数

测试结果：
scs的效果要比先生成标签再检索的效果好。

那直接将注释输入，然后生成token，效果会如何？



7.　既然从target上做手脚并不能提高准确度，那么还是从source入手：
第二阶段实验：
0. 训练输入注释，输出token的模型，提取encoder部分，通过两个encoder互相生成向量来对比。
1. 修改code2seq模型，加入一个encoder，试图用调用关系序列助理检索
2.　设计数据流序列，训练seq2seq模型，用数据流代替单纯的代码token序列进行训练。



先进行路径一的实验：
1. 数据准备：使用之前call dependency的数据，对摘要数据做个清理。
2. 模型修改：这次使用keras的模型进行训练，加一个encoder
3.　之后的步骤和scs相同，查看效果